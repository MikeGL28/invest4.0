import hashlib

from moexalgo import Ticker
from config import MOEX_API_KEY, TICKERS, NEWS_API_KEY, COMPANY_NAMES
import pandas as pd
import json
from datetime import datetime, timedelta
import os
from newsapi import NewsApiClient
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import time
from typing import List, Dict
import feedparser
import requests
# ===================== –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è =====================
newsapi = NewsApiClient(api_key=NEWS_API_KEY)
Ticker.TOKEN = MOEX_API_KEY

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
tokenizer = AutoTokenizer.from_pretrained("blanchefort/rubert-base-cased-sentiment")
model = AutoModelForSequenceClassification.from_pretrained("blanchefort/rubert-base-cased-sentiment")

# –î–∞—Ç—ã
today = datetime.today().strftime('%Y-%m-%d')
thirty_days_ago = (datetime.today() - timedelta(days=30)).strftime('%Y-%m-%d')

# –ü—É—Ç–∏
os.makedirs('data', exist_ok=True)
os.makedirs('news', exist_ok=True)
HISTORICAL_NEWS_PATH = 'news/Lenta_20_23.csv'
SENTIMENT_CACHE_PATH = 'news/sentiment_cache.json'
DATASET_CACHE_PATH = lambda ticker: f"data/{ticker}_ml_ready.csv"


# ===================== –§—É–Ω–∫—Ü–∏–∏ =====================
def load_sentiment_cache():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
    if os.path.exists(SENTIMENT_CACHE_PATH):
        with open(SENTIMENT_CACHE_PATH, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def save_sentiment_cache(cache):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
    with open(SENTIMENT_CACHE_PATH, 'w', encoding='utf-8') as f:
        json.dump(cache, f, indent=4, ensure_ascii=False)

def analyze_sentiment(text: str) -> str:
    """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RuBERT"""
    if not text or len(text.strip()) == 0:
        return "neutral"
    sentiment_cache = load_sentiment_cache()
    import hashlib
    cache_key = hashlib.sha256(text.encode()).hexdigest()
    if cache_key in sentiment_cache:
        return sentiment_cache[cache_key]

    try:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        label = torch.argmax(probs).item()
        sentiment_labels = ['negative', 'neutral', 'positive']
        sentiment = sentiment_labels[label]
        sentiment_cache[cache_key] = sentiment
        save_sentiment_cache(sentiment_cache)
        return sentiment
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏: {e}")
        return "neutral"

def fetch_news(ticker_name: str) -> list:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–∏–∫–µ—Ä—É (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π)"""
    try:
        all_articles = newsapi.get_everything(
            q=f"{ticker_name} OR {COMPANY_NAMES.get(ticker_name, ticker_name)}",
            from_param=thirty_days_ago,
            to=today,
            language='ru',
            sort_by='publishedAt',
            page_size=100
        )
        if all_articles['status'] == 'error':
            print(f"‚ö†Ô∏è NewsAPI error: {all_articles['message']}")
            if 'rate limit' in all_articles['message'].lower():
                print("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ —Å–±—Ä–æ—Å–∞ –ª–∏–º–∏—Ç–∞...")
                time.sleep(60)
            return []

        sentiment_cache = load_sentiment_cache()
        articles = []
        for article in all_articles.get('articles', []):
            title = article.get('title', '')
            description = article.get('description', '')
            content = f"{title}. {description}" if description else title
            published_at = article.get('publishedAt', '')
            if not published_at:
                continue

            news_date = published_at[:10]
            if news_date < thirty_days_ago or news_date > today:
                continue

            cache_key = hashlib.sha256(content.encode()).hexdigest()
            sentiment = sentiment_cache.get(cache_key, analyze_sentiment(content))
            if cache_key not in sentiment_cache:
                sentiment_cache[cache_key] = sentiment
                save_sentiment_cache(sentiment_cache)

            articles.append({
                'title': title,
                'description': description,
                'publishedAt': news_date,
                'sentiment': sentiment
            })
        print(f"üì∞ –ü–æ–ª—É—á–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {ticker_name}")
        return articles
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {ticker_name}: {str(e)}")
        return []

def load_historical_news(ticker_name: str) -> list:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ CSV"""
    try:
        df = pd.read_csv(HISTORICAL_NEWS_PATH, on_bad_lines='skip')
        required_cols = ['title', 'text', 'pubdate']
        if not all(col in df.columns for col in required_cols):
            raise KeyError(f"–í CSV-—Ñ–∞–π–ª–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {required_cols}")

        df['pubdate'] = pd.to_datetime(df['pubdate'], unit='s').dt.date
        today_date = datetime.strptime(today, '%Y-%m-%d').date()
        df = df[df['pubdate'] <= today_date]

        sentiment_cache = load_sentiment_cache()
        keywords = [ticker_name, COMPANY_NAMES.get(ticker_name, ticker_name)]
        filtered_news = []

        for _, row in df.iterrows():
            title = row['title']
            text = row.get('text', '')
            content = f"{title}. {text}" if text else title
            news_date = row['pubdate']
            valid_keywords = [kw for kw in keywords if isinstance(kw, str)]

            if any(kw.lower() in content.lower() for kw in valid_keywords):
                cache_key = hashlib.sha256(content.encode()).hexdigest()
                sentiment = sentiment_cache.get(cache_key, analyze_sentiment(content))
                if cache_key not in sentiment_cache:
                    sentiment_cache[cache_key] = sentiment
                    save_sentiment_cache(sentiment_cache)

                filtered_news.append({
                    'title': title,
                    'description': text[:200] if text else '',
                    'publishedAt': news_date,
                    'sentiment': sentiment
                })
        print(f"üì∞ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(filtered_news)} –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {ticker_name}")
        return filtered_news
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {str(e)}")
        return []

def combine_news(historical_news: list, recent_news: list) -> list:
    """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –∏ —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
    return historical_news + recent_news

# ===================== –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö =====================
def load_cached_dataset(ticker_name: str) -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    dataset_path = DATASET_CACHE_PATH(ticker_name)
    if os.path.exists(dataset_path):
        print(f"üß† –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è {ticker_name}")
        df = pd.read_csv(dataset_path)
        df['date'] = pd.to_datetime(df['date']).dt.date
        cols_to_drop = [col for col in df.columns if col.endswith('_x') or col.endswith('_y')]
        if cols_to_drop:
            print(f"üßπ –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {cols_to_drop}")
        #             df = df.drop(columns=cols_to_drop)
        return df
    return pd.DataFrame()


def get_last_date(df: pd.DataFrame) -> str:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –¥–∞—Ç—ã –∏–∑ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    if not df.empty:
        return df['date'].max().strftime('%Y-%m-%d')
    return '2020-01-01'


def update_price_data(df_cached: pd.DataFrame, ticker_name: str, last_date: str) -> pd.DataFrame:
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é —Ü–∏–∫–ª–∞, –ø–æ–∫–∞ –Ω–µ –¥–æ–π–¥—ë–º –¥–æ —Å–µ–≥–æ–¥–Ω—è"""
    ticker = Ticker(ticker_name)
    current_start = last_date
    all_new_candles = pd.DataFrame()
    max_iterations = 100
    iteration = 0

    while current_start < today and iteration < max_iterations:
        try:
            # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            candles = ticker.candles(start=current_start, end=today)
            if candles.empty:
                print(f"‚ùå –ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ {current_start}")
                # –î–≤–∏–≥–∞–µ–º –¥–∞—Ç—É –≤–ø–µ—Ä—ë–¥ –Ω–∞ 1 –¥–µ–Ω—å, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Å—Ç—Ä—è—Ç—å
                current_start_dt = datetime.strptime(current_start, '%Y-%m-%d') + timedelta(days=1)
                current_start = current_start_dt.strftime('%Y-%m-%d')
                iteration += 1
                continue

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≤–µ—á–µ–π
            candles['begin'] = pd.to_datetime(candles['begin'])
            candles['date'] = candles['begin'].dt.date
            candles = candles.drop_duplicates(subset='date', keep='last')
            candles = candles[['date', 'open', 'high', 'low', 'close', 'volume']]
            candles.columns = ['date', 'open_price', 'high_price', 'low_price', 'close_price', 'volume']

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            all_new_candles = pd.concat([all_new_candles, candles], ignore_index=True)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É –≤ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            last_fetched_date = candles['date'].max()

            # –°–ª–µ–¥—É—é—â–∞—è –¥–∞—Ç–∞ ‚Äî –Ω–∞ –¥–µ–Ω—å –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–ª—É—á–µ–Ω–Ω–æ–π
            next_start_dt = last_fetched_date + timedelta(days=1)
            current_start = next_start_dt.strftime('%Y-%m-%d')

            # –ï—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ —Å–µ–≥–æ–¥–Ω—è ‚Äî –≤—ã—Ö–æ–¥–∏–º
            if last_fetched_date >= datetime.strptime(today, '%Y-%m-%d').date():
                break

            iteration += 1

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker_name} —Å {current_start}: {e}")
            break

    if all_new_candles.empty:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–æ–≤—ã–µ —Ü–µ–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {ticker_name}")
        return df_cached

    # –û—á–∏—Å—Ç–∫–∞
    all_new_candles.drop_duplicates(subset='date', keep='last', inplace=True)
    all_new_candles.sort_values('date', inplace=True)

    print(f"üìà –ü–æ–ª—É—á–µ–Ω–æ {len(all_new_candles)} –Ω–æ–≤—ã—Ö —Å–≤–µ—á–µ–π –¥–ª—è {ticker_name}")

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∫—ç—à–µ–º
    if not df_cached.empty:
        last_date_dt = datetime.strptime(last_date, '%Y-%m-%d').date()
        df_filtered = df_cached[df_cached['date'] <= last_date_dt]
        updated_df = pd.concat([df_filtered, all_new_candles], ignore_index=True)
    else:
        updated_df = all_new_candles.copy()

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
    updated_df['date'] = pd.to_datetime(updated_df['date']).dt.date
    updated_df.drop_duplicates(subset='date', keep='last', inplace=True)
    updated_df.sort_values('date', inplace=True)

    return updated_df


# ===================== –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ñ–∏—á–µ–π =====================
def add_lagged_features(df: pd.DataFrame, lag: int = 5) -> pd.DataFrame:
    df = df.copy()
    for i in range(1, lag + 1):
        df[f'close_lag_{i}'] = df['close_price'].shift(i)
    # –ù–µ —É–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ
    if len(df) > 20:  # –•–≤–∞—Ç–∞–µ—Ç –¥–ª—è rolling –∏ lag
        df.dropna(inplace=True)
    else:
        df.fillna(0, inplace=True)  # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
    return df


def add_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
    df['SMA_5'] = df['close_price'].rolling(window=5).mean()
    df['SMA_10'] = df['close_price'].rolling(window=10).mean()
    df['RSI'] = compute_rsi(df['close_price'], window=14)
    df.dropna(inplace=True)
    return df


def compute_rsi(data: pd.Series, window: int = 14) -> pd.Series:
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ RSI"""
    delta = data.diff()
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    avg_gain = gain.rolling(window=window).mean()
    avg_loss = loss.rolling(window=window).mean()
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi


def create_target(df: pd.DataFrame) -> pd.DataFrame:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π: –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∑–∞ 15 –¥–Ω–µ–π –≤ %"""
    df['target_return'] = (df['close_price'].shift(-15) - df['close_price']) / df['close_price'] * 100
    df.dropna(subset=['target_return'], inplace=True)
    return df


def save_to_csv(df: pd.DataFrame, ticker_name: str) -> None:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ CSV"""
    df.to_csv(DATASET_CACHE_PATH(ticker_name), index=False)
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –¥–ª—è {ticker_name} –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {DATASET_CACHE_PATH(ticker_name)}")

# –ó–∞–≥–æ–ª–æ–≤–æ–∫, —á—Ç–æ–±—ã –Ω–µ –≤—ã–≥–ª—è–¥–µ—Ç—å –∫–∞–∫ –±–æ—Ç
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36'
}

def fetch_rbc_news(ticker_name: str) -> List[Dict]:
    """–ü–∞—Ä—Å–∏—Ç RSS –†–ë–ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º requests"""
    feed_url = "https://rssexport.rbc.ru/rbcnews/news/30/full.rss"
    return _parse_rss_with_requests(feed_url, ticker_name, "–†–ë–ö")

def fetch_interfax_news(ticker_name: str) -> List[Dict]:
    """–ü–∞—Ä—Å–∏—Ç RSS –ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º requests"""
    feed_url = "https://www.interfax.ru/rss.asp"
    return _parse_rss_with_requests(feed_url, ticker_name, "–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å")


def _parse_rss_with_requests(feed_url: str, ticker_name: str, source: str) -> List[Dict]:
    """–ü–∞—Ä—Å–∏–Ω–≥ RSS —á–µ—Ä–µ–∑ requests + feedparser —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    feed_url = feed_url.strip()

    try:
        response = requests.get(feed_url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        response.encoding = response.apparent_encoding

        feed = feedparser.parse(response.content)

        if feed.bozo:
            print(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {source}: {feed.bozo_exception}")

        articles = []
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ - –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π –∏–ª–∏ —Å–ø–∏—Å–∫–æ–º
        company_name = COMPANY_NAMES.get(ticker_name, ticker_name)

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = [ticker_name.lower()]

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã company_name
        if isinstance(company_name, str):
            # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞, –¥–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã
            keywords.append(company_name.lower())
            keywords.append(f"–∞–∫—Ü–∏–∏ {company_name}".lower())
            keywords.append(f"{company_name} –∫—É—Ä—Å".lower())
            keywords.append(f"{company_name} –¥–∏–≤–∏–¥–µ–Ω–¥—ã".lower())
        elif isinstance(company_name, list):
            # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç
            for name in company_name:
                if isinstance(name, str):
                    keywords.append(name.lower())
                    keywords.append(f"–∞–∫—Ü–∏–∏ {name}".lower())
                    keywords.append(f"{name} –∫—É—Ä—Å".lower())
                    keywords.append(f"{name} –¥–∏–≤–∏–¥–µ–Ω–¥—ã".lower())
        # # –î–ª—è GAZP –¥–æ–±–∞–≤–ª—è–µ–º "–≥–∞–∑–ø—Ä–æ–º" –∫–∞–∫ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
        # if ticker_name == "GAZP":
        #     keywords.append("–≥–∞–∑–ø—Ä–æ–º")
        #     keywords.append("–∞–∫—Ü–∏–∏ –≥–∞–∑–ø—Ä–æ–º")
        #     keywords.append("–≥–∞–∑–ø—Ä–æ–º –∫—É—Ä—Å")
        #     keywords.append("–≥–∞–∑–ø—Ä–æ–º –¥–∏–≤–∏–¥–µ–Ω–¥—ã")

        seen_titles = set()

        def to_string(value):
            if value is None:
                return ''
            if isinstance(value, str):
                return value
            if isinstance(value, (list, tuple)):
                return ' '.join(to_string(item) for item in value)
            if isinstance(value, dict):
                return ' '.join(to_string(v) for v in value.values())
            return str(value)

        for entry in feed.entries:
            title = to_string(entry.get('title', ''))
            summary = to_string(entry.get('summary', '') or entry.get('description', ''))
            published = to_string(entry.get('published', ''))
            link = to_string(entry.get('link', ''))

            if not title.strip():
                continue

            if title in seen_titles:
                continue

            # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            content_lower = (title + ' ' + summary).lower()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ª—é–±–æ–≥–æ –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            if any(kw in content_lower for kw in keywords):
                sentiment = analyze_sentiment(title + " " + summary)
                articles.append({
                    'title': title,
                    'description': summary[:200],
                    'publishedAt': published,
                    'url': link,
                    'sentiment': sentiment,
                    'source': source
                })
                seen_titles.add(title)

        print(f"üì∞ –ü–æ–ª—É—á–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {source} –¥–ª—è {ticker_name}")
        return articles

    except Exception as e:
        print(f"‚ùå –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {source}: {e}")
        import traceback
        traceback.print_exc()
        return []

# ===================== –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å =====================
for ticker_name in TICKERS:
    print(f"\nüîÑ –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è {ticker_name}...")

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    cached_df = load_cached_dataset(ticker_name)
    last_date = get_last_date(cached_df)
    print(f"‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å {last_date} –ø–æ {today}")

    # 2. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    price_df = update_price_data(cached_df, ticker_name, last_date)

    # 3. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π
    historical_news = load_historical_news(ticker_name)

    # üî• –ó–∞–º–µ–Ω—è–µ–º NewsAPI –Ω–∞ RSS-–ø–∞—Ä—Å–µ—Ä—ã
    print(f"üì° –ü–æ–ª—É—á–∞–µ–º —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è {ticker_name} –∏–∑ –†–ë–ö –∏ –ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å–∞...")
    rbc_news = fetch_rbc_news(ticker_name)
    interfax_news = fetch_interfax_news(ticker_name)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏
    recent_news = rbc_news + interfax_news

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏
    combined_news = combine_news(historical_news, recent_news)

    # 4. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ü–µ–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
    if not combined_news:
        print("üì∞ –ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ü–µ–Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
        merged_df = price_df.copy()
        # –Ø–≤–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ç–∏–ø–æ–º
        merged_df['sentiment_avg'] = 0.0
        merged_df['positive_news'] = 0.0
        merged_df['neutral_news'] = 0.0
        merged_df['negative_news'] = 0.0
    else:
        # –°–æ–∑–¥–∞—ë–º DataFrame –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π
        news_df = pd.DataFrame(combined_news)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–æ–Ω
        news_df['publishedAt'] = pd.to_datetime(news_df['publishedAt'], errors='coerce', utc=True)
        news_df['publishedAt'] = news_df['publishedAt'].dt.tz_localize(None)
        news_df['date'] = news_df['publishedAt'].dt.date

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –≤ —á–∏—Å–ª–æ–≤—É—é –º–µ—Ç—Ä–∏–∫—É
        sentiment_map = {'negative': -1, 'neutral': 0, 'positive': 1}
        news_df['sentiment_num'] = news_df['sentiment'].map(sentiment_map)

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–∞–º
        daily_sentiment = news_df.groupby('date').agg(
            sentiment_avg=('sentiment_num', 'mean'),
            positive_news=('sentiment', lambda x: (x == 'positive').sum()),
            neutral_news=('sentiment', lambda x: (x == 'neutral').sum()),
            negative_news=('sentiment', lambda x: (x == 'negative').sum())
        ).reset_index()

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Ü–µ–Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        merged_df = pd.merge(price_df, daily_sentiment, on='date', how='left')

        # üîë –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: —è–≤–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤
        for col in ['sentiment_avg', 'positive_news', 'neutral_news', 'negative_news']:
            if col in merged_df.columns:
                # –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø–æ–ª–Ω—è–µ–º NaN
                merged_df[col] = merged_df[col].fillna(0)
                # –ó–∞—Ç–µ–º —è–≤–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ float
                merged_df[col] = merged_df[col].astype(float)

        print(f"üìä –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –¥–ª—è {len(merged_df)} —Å—Ç—Ä–æ–∫")

    # üî• –î–æ–±–∞–≤–ª—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é: –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∑–∞ 15 –¥–Ω–µ–π –≤ %
    merged_df['target_return'] = (merged_df['close_price'].shift(-15) - merged_df['close_price']) / merged_df[
        'close_price'] * 100

    # 5. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ (—Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –¥–∞—Ç—ã –≤ –∫—ç—à–µ)
    try:
        last_date_dt = datetime.strptime(last_date, '%Y-%m-%d').date()
        new_data_df = merged_df[merged_df['date'] > last_date_dt].copy()
        print(f"üÜï –û—Ç–æ–±—Ä–∞–Ω–æ {len(new_data_df)} –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –ø–æ—Å–ª–µ {last_date_dt}")
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å last_date, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ: {e}")
        new_data_df = merged_df.copy()

    # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
    if new_data_df.empty:
        print(f"‚úÖ –ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker_name}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
        continue

    # 6. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    new_data_df = add_lagged_features(new_data_df)
    new_data_df = add_technical_indicators(new_data_df)
    # –£–¥–∞–ª—è–µ–º create_target, —Ç–∞–∫ –∫–∞–∫ target —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω –≤—ã—à–µ
    # new_data_df = create_target(new_data_df)  # ‚ùå –£–¥–∞–ª—è–µ–º —ç—Ç—É —Å—Ç—Ä–æ–∫—É

    if new_data_df.empty:
        print(f"‚ùå –ü–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {ticker_name} –ø—É—Å—Ç—ã")
        continue

    # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    final_df = pd.concat([cached_df, new_data_df], ignore_index=True)
    final_df['date'] = pd.to_datetime(final_df['date']).dt.date
    final_df.drop_duplicates(subset='date', keep='last', inplace=True)
    final_df.sort_values('date', inplace=True)

    save_to_csv(final_df, ticker_name)

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –≤—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –Ω–æ–≤–æ—Å—Ç–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
    print(f"üîç –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {ticker_name}:")
    print(final_df[['sentiment_avg', 'positive_news', 'neutral_news', 'negative_news']].describe())

print("\nüéâ –í—Å–µ –¥–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω—ã!")