# train_models.py
import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import joblib
import json
from datetime import datetime
from config import TICKERS
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import lightgbm as lgb
import xgboost as xgb
import shap  # –î–æ–±–∞–≤–ª–µ–Ω –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
import math
# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–æ–∫
os.makedirs("models", exist_ok=True)
os.makedirs("results", exist_ok=True)
os.makedirs("results/explanations", exist_ok=True)

# ===================== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ =====================
TARGET = 'target_return'  # –¢–µ–ø–µ—Ä—å –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤ %
FEATURES = [
               'close_price', 'open_price', 'high_price', 'low_price', 'volume',
               'rsi', 'macd', 'macd_signal', 'bollinger_upper', 'bollinger_lower',
               'sentiment_avg', 'positive_news', 'neutral_news', 'negative_news'
           ] + [f'close_lag_{i}' for i in range(1, 6)]

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –≤–µ—Å–∞ (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫–∞–∫ fallback)
BASELINE_WEIGHTS = {
    'lgb': 0.3,
    'xgb': 0.5,
    'lstm': 0.2
}

LGB_PARAMS = {
    'objective': 'regression',
    'metric': 'rmse',
    'boosting_type': 'gbdt',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.9,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': -1,
    'seed': 42
}

LSTM_SEQ_LEN = 10
LSTM_EPOCHS = 100
LSTM_BATCH_SIZE = 32
LSTM_LR = 0.001


class LSTMModel(nn.Module):
    """
    LSTM –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ num_layers > 1).
    """

    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        # LSTM —Å–ª–æ–π —Å dropout
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0  # PyTorch —Ç—Ä–µ–±—É–µ—Ç num_layers > 1 –¥–ª—è dropout
        )
        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π –¥–ª—è –≤—ã—Ö–æ–¥–∞
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥.
        :param x: —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (batch_size, seq_len, input_size)
        :return: –ø—Ä–æ–≥–Ω–æ–∑ (batch_size, 1)
        """
        # –í—ã—Ö–æ–¥ LSTM: (batch_size, seq_len, hidden_size)
        lstm_out, _ = self.lstm(x)
        # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —à–∞–≥
        last_out = lstm_out[:, -1, :]  # (batch_size, hidden_size)
        # –ü—Ä–æ–≥–Ω–æ–∑
        out = self.fc(last_out)  # (batch_size, 1)
        return out


# ===================== –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ =====================
def load_data(ticker):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV"""
    path = f"data/{ticker}_ml_ready.csv"
    if os.path.exists(path):
        df = pd.read_csv(path)
        df['date'] = pd.to_datetime(df['date'])
        df.sort_values('date', inplace=True)
        df.drop_duplicates(subset='date', keep='last', inplace=True)
        print(f"üß† –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è {ticker} (—Å—Ç—Ä–æ–∫: {len(df)})")
        return df
    return None


def create_target(df):
    """–°–æ–∑–¥–∞—ë–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é: –ª–æ–≥-–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∑–∞ 15 –¥–Ω–µ–π"""
    df = df.copy()
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –Ω—É–ª–µ–≤—ã—Ö –∏–ª–∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–Ω
    if (df['close_price'] <= 0).any():
        print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω—É–ª–µ–≤—ã–µ –∏–ª–∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Ü–µ–Ω—ã")
        df['close_price'] = df['close_price'].clip(lower=0.01)

    df['target_log_return'] = np.log(df['close_price'].shift(-15) / df['close_price'])
    return df


def safe_rsi(ser):
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞—Å—á—ë—Ç RSI —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫—Ä–∞–π–Ω–∏—Ö —Å–ª—É—á–∞–µ–≤"""
    if len(ser) < 2:
        return 50
    changes = np.diff(ser)
    up = changes[changes > 0].sum()
    down = -changes[changes < 0].sum()
    if down == 0:
        return 100
    if up == 0:
        return 0
    rs = up / down
    return 100 - (100 / (1 + rs))


def prepare_features(df):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –ª–∞–≥–∏, –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã, —Ü–µ–ª—å
    –í–ê–ñ–ù–û: –í—Å–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Å–º–µ—â–µ–Ω—ã –Ω–∞ 1 —à–∞–≥ –Ω–∞–∑–∞–¥ (shift(1)), —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö.
    """
    df = df.copy()
    df.sort_values('date', inplace=True)
    df = df.reset_index(drop=True)
    # 1. –õ–∞–≥–∏ —Ü–µ–Ω
    for i in range(1, 6):
        df[f'close_lag_{i}'] = df['close_price'].shift(i)
    # 2. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã ‚Äî –≤—Å–µ —Å–æ —Å–º–µ—â–µ–Ω–∏–µ–º
    df['rsi'] = df['close_price'].rolling(14).apply(safe_rsi, raw=True).shift(1)
    df['ma20'] = df['close_price'].rolling(20).mean().shift(1)
    df['ma50'] = df['close_price'].rolling(50).mean().shift(1)
    df['macd'] = (df['ma20'] - df['ma50']).shift(1)
    df['macd_signal'] = df['macd'].rolling(9).mean().shift(1)
    df['bollinger_std'] = df['close_price'].rolling(20).std().shift(1)
    df['bollinger_upper'] = (df['ma20'] + 2 * df['bollinger_std']).shift(1)
    df['bollinger_lower'] = (df['ma20'] - 2 * df['bollinger_std']).shift(1)
    # 3. –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
    df = create_target(df)
    # 4. –£–¥–∞–ª–µ–Ω–∏–µ NaN
    feature_cols = [col for col in FEATURES if col in df.columns]
    required_cols = feature_cols + [TARGET]
    if not all(col in df.columns for col in required_cols):
        missing = [col for col in required_cols if col not in df.columns]
        raise ValueError(f"–ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –∫–æ–ª–æ–Ω–æ–∫: {missing}")
    df.dropna(subset=required_cols, inplace=True)
    X = df[feature_cols].copy()
    y = df[TARGET].copy()
    dates = df['date'].copy()
    return X, y, dates


def save_scaler(scaler, ticker):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ StandardScaler"""
    os.makedirs('models', exist_ok=True)
    joblib.dump(scaler, f'models/{ticker}_scaler.pkl')


def create_sequences(data, seq_length):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è LSTM"""
    sequences = []
    for i in range(len(data) - seq_length + 1):
        sequences.append(data[i:i + seq_length])
    return np.array(sequences)


def calculate_adaptive_weights(ticker, lgb_dir_acc, xgb_dir_acc, lstm_dir_acc, alpha=0.7):
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—é (directional accuracy)

    :param alpha: –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è (0.7 –æ–∑–Ω–∞—á–∞–µ—Ç 70% –Ω–æ–≤–æ–≥–æ –≤–µ—Å–∞, 30% –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ)
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –≤–µ—Å–∞
    weights_path = f'models/{ticker}_ensemble_weights.json'
    prev_weights = None

    if os.path.exists(weights_path):
        try:
            with open(weights_path, 'r') as f:
                prev_weights = json.load(f)
        except:
            pass

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (–¥–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–µ —á–∏—Å–ª–æ, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å 0)
    valid_models = []
    weights = []

    if not np.isnan(lgb_dir_acc):
        valid_models.append('lgb')
        weights.append(lgb_dir_acc)
    if not np.isnan(xgb_dir_acc):
        valid_models.append('xgb')
        weights.append(xgb_dir_acc)
    if not np.isnan(lstm_dir_acc):
        valid_models.append('lstm')
        weights.append(lstm_dir_acc)

    # –ï—Å–ª–∏ –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –≤–µ—Å–∞
    if not valid_models:
        print(f"‚ö†Ô∏è –í—Å–µ –º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç nan –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è {ticker}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –≤–µ—Å–∞")
        return BASELINE_WEIGHTS.copy()

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞
    total = sum(weights) + 1e-6
    current_weights = {model: weight / total for model, weight in zip(valid_models, weights)}

    # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –≤–µ—Å–∞, –ø—Ä–∏–º–µ–Ω—è–µ–º —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
    adaptive_weights = BASELINE_WEIGHTS.copy()  # –ù–∞—á–∏–Ω–∞–µ–º —Å–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –≤–µ—Å–æ–≤

    if prev_weights:
        for model in adaptive_weights:
            if model in current_weights:
                adaptive_weights[model] = alpha * current_weights[model] + (1 - alpha) * prev_weights.get(model,
                                                                                                          BASELINE_WEIGHTS[
                                                                                                              model])
            else:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –≤–µ—Å, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ —Å–µ–π—á–∞—Å
                adaptive_weights[model] = prev_weights.get(model, BASELINE_WEIGHTS[model])
    else:
        for model in adaptive_weights:
            if model in current_weights:
                adaptive_weights[model] = current_weights[model]

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º, —á—Ç–æ–±—ã —Å—É–º–º–∞ –≤–µ—Å–æ–≤ = 1
    total_weights = sum(adaptive_weights.values())
    if total_weights > 0:
        for model in adaptive_weights:
            adaptive_weights[model] /= total_weights

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ä–∞–∑–∞
    with open(weights_path, 'w') as f:
        json.dump(adaptive_weights, f, indent=4)

    return adaptive_weights


def load_ensemble_weights(ticker):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –∏–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ"""
    weights_path = f'models/{ticker}_ensemble_weights.json'
    if os.path.exists(weights_path):
        try:
            with open(weights_path, 'r') as f:
                return json.load(f)
        except:
            pass
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –≤–µ—Å–∞ –∫–∞–∫ fallback
    return BASELINE_WEIGHTS.copy()


def explain_prediction(ticker, model, scaler, X_full, days_ahead=0):
    """–û–±—ä—è—Å–Ω—è–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑ —Å –ø–æ–º–æ—â—å—é SHAP –∑–Ω–∞—á–µ–Ω–∏–π"""
    try:
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_scaled = scaler.transform(X_full)
        last_instance = X_scaled[-1].reshape(1, -1)

        # –°–æ–∑–¥–∞–µ–º explainer (–¥–ª—è –¥–µ—Ä–µ–≤—å–µ–≤)
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(last_instance)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
        try:
            shap.initjs()  # –ú–æ–∂–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å –æ—à–∏–±–∫—É, –µ—Å–ª–∏ IPython –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
            force_plot = shap.force_plot(
                explainer.expected_value,
                shap_values[0],
                features=last_instance[0],
                feature_names=X_full.columns.tolist(),
                show=False
            )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ HTML
            os.makedirs(f"results/explanations", exist_ok=True)
            shap.save_html(f"results/explanations/{ticker}_shap_{days_ahead}.html", force_plot)
        except Exception as e:
            print(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å HTML-–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–ª—è {ticker}: {e}")

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        feature_importance = pd.Series(shap_values[0], index=X_full.columns)
        top_features = feature_importance.abs().sort_values(ascending=False).head(5).index.tolist()

        return {
            'top_features': top_features,
            'explanation_path': f"results/explanations/{ticker}_shap_{days_ahead}.html" if os.path.exists(
                f"results/explanations/{ticker}_shap_{days_ahead}.html") else None
        }
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ SHAP –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –¥–ª—è {ticker}: {e}")
        return {
            'top_features': [],
            'explanation_path': None
        }


def find_similar_patterns(ticker, X_full, n_similar=3):
    """–ù–∞—Ö–æ–¥–∏—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–∏–æ–¥—ã —Å –ø–æ—Ö–æ–∂–∏–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏"""
    try:
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
        X_scaled = StandardScaler().fit_transform(X_full[FEATURES])

        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∫ –∑–∞–ø—Ä–æ—Å
        query = X_scaled[-1]

        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
        distances = np.linalg.norm(X_scaled[:-15] - query, axis=1)  # –∏—Å–∫–ª—é—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 15 –¥–Ω–µ–π

        # –ù–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à–∏–µ –∞–Ω–∞–ª–æ–≥–∏
        similar_indices = np.argsort(distances)[:n_similar]

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–æ –ø–æ—Å–ª–µ —ç—Ç–∏—Ö –º–æ–º–µ–Ω—Ç–æ–≤
        similar_periods = []
        for idx in similar_indices:
            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∏–Ω–¥–µ–∫—Å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–∞
            if idx + 15 >= len(X_full):
                continue

            future_return = np.log(X_full['close_price'].iloc[idx + 15] / X_full['close_price'].iloc[idx])

            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –ø–æ–ª—É—á–∞–µ–º –¥–∞—Ç—É –ø—Ä–∞–≤–∏–ª—å–Ω–æ
            date_value = X_full.index[idx]
            if isinstance(date_value, (int, np.integer)):
                # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å —á–∏—Å–ª–æ–≤–æ–π, –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –¥–∞—Ç—É –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ DataFrame
                if 'date' in X_full.columns:
                    date_str = X_full['date'].iloc[idx]
                else:
                    date_str = f"–î–∞—Ç–∞ {idx}"
            else:
                date_str = date_value

            similar_periods.append({
                'date': date_str,
                'similarity': 1 / (1 + distances[idx]),
                'future_return': future_return,
                'trend': 'up' if future_return > 0 else 'down'
            })

        return similar_periods
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –∞–Ω–∞–ª–æ–≥–æ–≤ –¥–ª—è {ticker}: {e}")
        return []


def train_or_finetune_lstm(ticker, X_train_seq, y_train, X_val_seq, y_val, input_size, num_epochs=100):
    """–î–æ–æ–±—É—á–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model_path = f'models/{ticker}_lstm.pth'
    model = LSTMModel(input_size=input_size, hidden_size=64, num_layers=2, dropout=0.2).to(device)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if os.path.exists(model_path):
        try:
            model.load_state_dict(torch.load(model_path, map_location=device))
            print(f"üîÅ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ LSTM –º–æ–¥–µ–ª—å –¥–ª—è {ticker}, –¥–æ–æ–±—É—á–∞–µ–º...")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è {ticker}: {e}. –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é.")
    else:
        print(f"üÜï –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è LSTM –º–æ–¥–µ–ª—å –¥–ª—è {ticker}")

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø—É—Å—Ç—ã–µ
    if len(X_train_seq) == 0 or len(X_val_seq) == 0:
        print(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LSTM –¥–ª—è {ticker}")
        return model, device

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã
    X_train_tensor = torch.FloatTensor(X_train_seq).to(device)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device).unsqueeze(1)
    X_val_tensor = torch.FloatTensor(X_val_seq).to(device)
    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device).unsqueeze(1)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç
    if X_train_tensor.shape[0] != y_train_tensor.shape[0]:
        min_len = min(X_train_tensor.shape[0], y_train_tensor.shape[0])
        X_train_tensor = X_train_tensor[:min_len]
        y_train_tensor = y_train_tensor[:min_len]
        print(f"‚ö†Ô∏è –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ X_train –∏ y_train –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç –¥–ª—è {ticker}. –û–±—Ä–µ–∑–∞–µ–º –¥–æ {min_len} –æ–±—Ä–∞–∑—Ü–æ–≤.")

    if X_val_tensor.shape[0] != y_val_tensor.shape[0]:
        min_len = min(X_val_tensor.shape[0], y_val_tensor.shape[0])
        X_val_tensor = X_val_tensor[:min_len]
        y_val_tensor = y_val_tensor[:min_len]
        print(f"‚ö†Ô∏è –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ X_val –∏ y_val –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç –¥–ª—è {ticker}. –û–±—Ä–µ–∑–∞–µ–º –¥–æ {min_len} –æ–±—Ä–∞–∑—Ü–æ–≤.")

    best_loss = float('inf')
    patience = 30
    trigger = 0

    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()
        y_pred = model(X_train_tensor)
        loss = criterion(y_pred, y_train_tensor)
        loss.backward()
        optimizer.step()

        model.eval()
        with torch.no_grad():
            val_pred = model(X_val_tensor)
            val_loss = criterion(val_pred, y_val_tensor).item()

        if epoch % 10 == 0:
            print(f"Epoch {epoch}, Val Loss: {val_loss:.6f}")

        if val_loss < best_loss:
            best_loss = val_loss
            trigger = 0
        else:
            trigger += 1
            if trigger >= patience:
                print(f"üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —ç–ø–æ—Ö–µ {epoch} –∏–∑-–∑–∞ early stopping")
                break

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¢–û–õ–¨–ö–û –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
    torch.save(model.state_dict(), model_path)
    print(f"üíæ –ú–æ–¥–µ–ª—å LSTM –¥–ª—è {ticker} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    return model, device


def predict_next_15_days(ticker, model_lgb, model_xgb, model_lstm, device, scaler, X_full):
    """–ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 15 –¥–Ω–µ–π –≤–ø–µ—Ä—ë–¥ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ –∏ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–º–∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏"""
    model_lstm.eval()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞
    ensemble_weights = load_ensemble_weights(ticker)

    # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º X_full —Å –ø–æ–º–æ—â—å—é scaler
    current_X = scaler.transform(X_full)
    feature_names = X_full.columns.tolist()

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–Ω—é—é —Ü–µ–Ω—É
    initial_price = X_full['close_price'].iloc[-1]

    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ 15-–¥–Ω–µ–≤–Ω–æ–π –ª–æ–≥-–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
    flat_input = current_X[-1].reshape(1, -1)
    assert flat_input.shape[1] == len(FEATURES), f"–û–∂–∏–¥–∞–ª–æ—Å—å {len(FEATURES)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø–æ–ª—É—á–µ–Ω–æ {flat_input.shape[1]}"

    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    try:
        lgb_pred = model_lgb.predict(flat_input)[0]
    except:
        lgb_pred = 0
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ LightGBM –¥–ª—è {ticker}")

    try:
        xgb_pred = model_xgb.predict(flat_input)[0]
    except:
        xgb_pred = 0
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ XGBoost –¥–ª—è {ticker}")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è LSTM
    if len(current_X) >= LSTM_SEQ_LEN:
        seq = current_X[-LSTM_SEQ_LEN:].reshape(1, LSTM_SEQ_LEN, -1)
        seq_tensor = torch.FloatTensor(seq).to(device)
        with torch.no_grad():
            try:
                lstm_pred = model_lstm(seq_tensor).cpu().numpy().flatten()[0]
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ LSTM –¥–ª—è {ticker}: {e}")
                lstm_pred = 0
    else:
        lstm_pred = 0
        print(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LSTM –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è {ticker}")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞, —É—á–∏—Ç—ã–≤–∞—è –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ—à–∏–±–∫–∏
    valid_models = []
    weighted_sum = 0
    total_weight = 0

    if not np.isnan(lgb_pred) and not np.isinf(lgb_pred):
        valid_models.append('lgb')
        weighted_sum += ensemble_weights.get('lgb', BASELINE_WEIGHTS['lgb']) * lgb_pred
        total_weight += ensemble_weights.get('lgb', BASELINE_WEIGHTS['lgb'])

    if not np.isnan(xgb_pred) and not np.isinf(xgb_pred):
        valid_models.append('xgb')
        weighted_sum += ensemble_weights.get('xgb', BASELINE_WEIGHTS['xgb']) * xgb_pred
        total_weight += ensemble_weights.get('xgb', BASELINE_WEIGHTS['xgb'])

    if not np.isnan(lstm_pred) and not np.isinf(lstm_pred):
        valid_models.append('lstm')
        weighted_sum += ensemble_weights.get('lstm', BASELINE_WEIGHTS['lstm']) * lstm_pred
        total_weight += ensemble_weights.get('lstm', BASELINE_WEIGHTS['lstm'])

    # –ï—Å–ª–∏ –µ—Å—Ç—å –≤–∞–ª–∏–¥–Ω—ã–µ –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
    if total_weight > 0:
        ensemble_pred = weighted_sum / total_weight
    else:
        # –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –¥–∞–ª–∏ –æ—à–∏–±–∫—É, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        ensemble_pred = 0
        print(f"‚ö†Ô∏è –í—Å–µ –º–æ–¥–µ–ª–∏ –¥–∞–ª–∏ –æ—à–∏–±–∫—É –¥–ª—è {ticker}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑")

    # –û—Ü–µ–Ω–∫–∞ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
    daily_returns = X_full['close_price'].pct_change().dropna()
    if len(daily_returns) > 0:
        volatility = daily_returns.std() * np.sqrt(252)  # –≥–æ–¥–æ–≤–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
    else:
        volatility = 0.3  # –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (30%)

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
    max_reasonable_log_return = np.log(1 + 3 * volatility * np.sqrt(15 / 252))
    min_reasonable_log_return = np.log(1 - 3 * volatility * np.sqrt(15 / 252))

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤—ã—Ö–æ–¥—è—Ç –ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞ —Ä–∞–∑—É–º–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã
    if ensemble_pred > max_reasonable_log_return:
        print(
            f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è {ticker} ({ensemble_pred:.4f}) –ø—Ä–µ–≤—ã—à–∞–µ—Ç —Ä–∞–∑—É–º–Ω—ã–π –ø—Ä–µ–¥–µ–ª ({max_reasonable_log_return:.4f}). –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º.")
        ensemble_pred = max_reasonable_log_return
    elif ensemble_pred < min_reasonable_log_return:
        print(
            f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è {ticker} ({ensemble_pred:.4f}) –Ω–∏–∂–µ —Ä–∞–∑—É–º–Ω–æ–≥–æ –ø—Ä–µ–¥–µ–ª–∞ ({min_reasonable_log_return:.4f}). –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º.")
        ensemble_pred = min_reasonable_log_return

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –¥–ª—è 15-–¥–Ω–µ–≤–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞
    ci_range = volatility * np.sqrt(15 / 252) * 1.645  # 90% –î–ò –¥–ª—è 15 –¥–Ω–µ–π
    final_price = initial_price * np.exp(ensemble_pred)
    ci_lower = initial_price * np.exp(ensemble_pred - ci_range)
    ci_upper = initial_price * np.exp(ensemble_pred + ci_range)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
    price_forecast = np.linspace(initial_price, final_price, 15)
    ci_lower_values = np.linspace(initial_price, ci_lower, 15)
    ci_upper_values = np.linspace(initial_price, ci_upper, 15)

    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–∂–∏–¥–∞–µ–º—É—é –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
    expected_return = (final_price / initial_price - 1) * 100
    expected_return_range = (
        (ci_lower / initial_price - 1) * 100,
        (ci_upper / initial_price - 1) * 100
    )

    # –†–∞—Å—á—ë—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ä–æ—Å—Ç–∞
    def calc_growth_prob(model_pred):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
        z_score = model_pred / (volatility * np.sqrt(15 / 252))
        # –ü—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        return max(0, min(100, (0.5 + 0.5 * math.erf(z_score / math.sqrt(2))) * 100))

    lgb_growth_prob = calc_growth_prob(lgb_pred) if 'lgb' in valid_models else 50.0
    xgb_growth_prob = calc_growth_prob(xgb_pred) if 'xgb' in valid_models else 50.0
    lstm_growth_prob = calc_growth_prob(lstm_pred) if 'lstm' in valid_models else 50.0

    # –ê–Ω—Å–∞–º–±–ª–µ–≤–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
    ensemble_growth_prob = calc_growth_prob(ensemble_pred)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    return {
        'price_forecast': price_forecast,
        'current_price': initial_price,
        'ci_lower': ci_lower_values,
        'ci_upper': ci_upper_values,
        'expected_return': expected_return,
        'expected_return_range': expected_return_range,
        'growth_probs': {
            'lgb': lgb_growth_prob,
            'xgb': xgb_growth_prob,
            'lstm': lstm_growth_prob,
            'ensemble': ensemble_growth_prob
        }
    }


# ===================== –û–±—É—á–µ–Ω–∏–µ LightGBM =====================
def train_or_finetune_lgb(ticker, X_train, y_train, X_val, y_val):
    model_path = f"models/{ticker}_lgb.txt"
    init_model = model_path if os.path.exists(model_path) else None
    train_data = lgb.Dataset(X_train, label=y_train)
    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)
    print(f"üìà {'–î–æ–æ–±—É—á–∞–µ–º' if init_model else '–û–±—É—á–∞–µ–º'} LightGBM –¥–ª—è {ticker}...")
    model = lgb.train(
        LGB_PARAMS,
        train_data,
        num_boost_round=1000,
        valid_sets=[train_data, val_data],
        init_model=init_model,
        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]
    )
    model.save_model(model_path)
    return model


# ===================== –û–±—É—á–µ–Ω–∏–µ XGBoost =====================
def train_or_finetune_xgb(ticker, X_train, y_train, X_val, y_val):
    model_path = f"models/{ticker}_xgb.pkl"
    params = {
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'max_depth': 6,
        'learning_rate': 0.01,
        'subsample': 0.8,
        'colsample_bytree': 0.9,
        'random_state': 42
    }
    dtrain = xgb.DMatrix(X_train, label=y_train)
    dval = xgb.DMatrix(X_val, label=y_val)
    print(f"üìà {'–î–æ–æ–±—É—á–∞–µ–º' if os.path.exists(model_path) else '–û–±—É—á–∞–µ–º'} XGBoost –¥–ª—è {ticker}...")
    if os.path.exists(model_path):
        booster = xgb.train(
            params=params,
            dtrain=dtrain,
            num_boost_round=1000,
            evals=[(dtrain, "train"), (dval, "val")],
            early_stopping_rounds=100,
            xgb_model=model_path
        )
    else:
        booster = xgb.train(
            params=params,
            dtrain=dtrain,
            num_boost_round=1000,
            evals=[(dtrain, "train"), (dval, "val")],
            early_stopping_rounds=100
        )
    booster.save_model(model_path)
    model = xgb.XGBRegressor(**params)
    model._Booster = booster
    return model


# ===================== –°–∫–∞–ª–µ—Ä –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ =====================
def load_scaler(ticker):
    path = f"models/{ticker}_scaler.pkl"
    if os.path.exists(path):
        scaler = joblib.load(path)
        print(f"üîÅ –ó–∞–≥—Ä—É–∂–µ–Ω —Å–∫–∞–ª–µ—Ä –¥–ª—è {ticker}")
        return scaler
    return None


def save_scaler(scaler, ticker):
    joblib.dump(scaler, f"models/{ticker}_scaler.pkl")
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω —Å–∫–∞–ª–µ—Ä –¥–ª—è {ticker}")


def save_meta(ticker, last_date, data_length):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏"""
    os.makedirs('models', exist_ok=True)
    meta_path = f'models/{ticker}_meta.json'
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º Timestamp –≤ —Å—Ç—Ä–æ–∫—É
    if isinstance(last_date, pd.Timestamp):
        last_date_str = last_date.strftime('%Y-%m-%d %H:%M:%S')
    else:
        last_date_str = str(last_date)
    meta = {
        'ticker': ticker,
        'last_update': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'last_date_in_data': last_date_str,
        'data_length': data_length,
        'model_version': '1.1'  # –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
    }
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump(meta, f, indent=4, ensure_ascii=False)
    print(f"üíæ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è {ticker} —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")


def main():
    all_predictions = []
    for ticker in TICKERS:
        print(f"üöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–∏–∫–µ—Ä–∞: {ticker}")
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = load_data(ticker)
        if df is None or df.empty or len(df) < 50:
            continue
        # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X, y, dates = prepare_features(df)
        if X.empty or len(X) < 50:
            continue
        # 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train / val / test
        split1 = int(len(X) * 0.7)
        split2 = int(len(X) * 0.85)
        X_train, X_val, X_test = X[:split1], X[split1:split2], X[split2:]
        y_train, y_val, y_test = y[:split1], y[split1:split2], y[split2:]
        # 4. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è ‚Äî —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º DataFrame –¥–ª—è feature names
        scaler = StandardScaler()
        X_full_scaled = pd.DataFrame(
            scaler.fit_transform(X),
            columns=X.columns,
            index=X.index
        )
        X_train_scaled = X_full_scaled.iloc[:split1]
        X_val_scaled = X_full_scaled.iloc[split1:split2]
        X_test_scaled = X_full_scaled.iloc[split2:]
        save_scaler(scaler, ticker)
        save_meta(ticker, dates.iloc[-1], len(X))
        # 5. –û–±—É—á–µ–Ω–∏–µ LightGBM –∏ XGBoost
        model_lgb = train_or_finetune_lgb(ticker, X_train_scaled.values, y_train, X_val_scaled.values, y_val)
        model_xgb = train_or_finetune_xgb(ticker, X_train_scaled.values, y_train, X_val_scaled.values, y_val)
        # 6. LSTM: –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        X_seq = create_sequences(X_full_scaled.values, LSTM_SEQ_LEN)
        train_end = split1 - LSTM_SEQ_LEN
        val_end = split2 - LSTM_SEQ_LEN
        X_train_seq = X_seq[:train_end]
        X_val_seq = X_seq[train_end:val_end]
        y_train_seq = y.iloc[LSTM_SEQ_LEN:split1].values
        y_val_seq = y.iloc[split1:split2].values
        model_lstm, device = train_or_finetune_lstm(
            ticker, X_train_seq, y_train_seq, X_val_seq, y_val_seq, input_size=X.shape[1]
        )
        # 7. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ —Ç–µ—Å—Ç–µ
        dir_acc_lgb = dir_acc_xgb = dir_acc_lstm = baseline_dir_acc = np.nan

        try:
            # LightGBM
            y_pred_lgb = model_lgb.predict(X_test_scaled.values)
            rmse_lgb = np.sqrt(mean_squared_error(y_test, y_pred_lgb))
            mae_lgb = mean_absolute_error(y_test, y_pred_lgb)
            dir_acc_lgb = ((y_pred_lgb > 0) == (y_test > 0)).mean()
            print(f"üìä {ticker} | LGB: RMSE={rmse_lgb:.3f}, MAE={mae_lgb:.3f}, DirAcc={dir_acc_lgb:.3f}")

            # XGBoost
            y_pred_xgb = model_xgb.predict(X_test_scaled.values)
            rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
            mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
            dir_acc_xgb = ((y_pred_xgb > 0) == (y_test > 0)).mean()
            print(f"üìä {ticker} | XGB: RMSE={rmse_xgb:.3f}, MAE={mae_xgb:.3f}, DirAcc={dir_acc_xgb:.3f}")

            # LSTM
            if len(X_seq) > 0 and len(y_test) >= LSTM_SEQ_LEN:
                X_test_seq = create_sequences(X_test_scaled.values, LSTM_SEQ_LEN)
                y_test_lstm = y_test.iloc[LSTM_SEQ_LEN:].values

                if len(X_test_seq) == len(y_test_lstm):
                    X_test_seq_tensor = torch.FloatTensor(X_test_seq).to(device)
                    model_lstm.eval()
                    with torch.no_grad():
                        y_pred_lstm = model_lstm(X_test_seq_tensor).cpu().numpy().flatten()
                    rmse_lstm = np.sqrt(mean_squared_error(y_test_lstm, y_pred_lstm))
                    mae_lstm = mean_absolute_error(y_test_lstm, y_pred_lstm)
                    dir_acc_lstm = ((y_pred_lstm > 0) == (y_test_lstm > 0)).mean()
                    print(f"üìä {ticker} | LSTM: RMSE={rmse_lstm:.3f}, MAE={mae_lstm:.3f}, DirAcc={dir_acc_lstm:.3f}")
                else:
                    print(
                        f"‚ö†Ô∏è –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –¥–ª—è LSTM –≤ {ticker}: {len(X_test_seq)} vs {len(y_test_lstm)}")
            else:
                print(f"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LSTM —Ç–µ—Å—Ç–∞ –≤ {ticker}")

            # –ë–µ–π–∑–ª–∞–π–Ω: –ø—Ä–æ—Å—Ç–æ–π —Ç—Ä–µ–Ω–¥ (–¥–Ω–µ–≤–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å)
            baseline_preds = X['close_price'].pct_change().shift(-1)
            baseline_preds = baseline_preds.iloc[split2:-15]  # –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Å y_test
            y_test_aligned = y_test.iloc[:-15]
            if len(baseline_preds) == len(y_test_aligned):
                baseline_dir_acc = ((baseline_preds > 0) == (y_test_aligned > 0)).mean()
                print(f"üìä {ticker} | Baseline: DirAcc={baseline_dir_acc:.3f}")
            else:
                print(f"‚ö†Ô∏è –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –¥–ª—è –±–µ–π–∑–ª–∞–π–Ω–∞ –≤ {ticker}")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ —Ç–µ—Å—Ç–µ –¥–ª—è {ticker}: {e}")

        # 7.5. –†–∞—Å—á–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –≤–µ—Å–æ–≤
        try:
            adaptive_weights = calculate_adaptive_weights(
                ticker,
                dir_acc_lgb,
                dir_acc_xgb,
                dir_acc_lstm
            )
            print(f"‚öñÔ∏è –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è {ticker}: LGB={adaptive_weights['lgb']:.2f}, "
                  f"XGB={adaptive_weights['xgb']:.2f}, LSTM={adaptive_weights['lstm']:.2f}")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è {ticker}: {e}, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ")

        # 8. –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 15 –¥–Ω–µ–π
        try:
            growth_probs = predict_next_15_days(ticker, model_lgb, model_xgb, model_lstm, device, scaler, X)

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è SHAP –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
            explanation = {'top_features': []}
            try:
                explanation = explain_prediction(ticker, model_xgb, scaler, X)
                if explanation['top_features']:
                    print(f"üîç {ticker}: –ü—Ä–æ–≥–Ω–æ–∑ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å: {', '.join(explanation['top_features'])}")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å SHAP –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–ª—è {ticker}: {e}")

            # –ü–æ–∏—Å–∫ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –∞–Ω–∞–ª–æ–≥–æ–≤
            similar_patterns = []
            try:
                similar_patterns = find_similar_patterns(ticker, X)
                if similar_patterns:
                    print(f"üîç –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –∞–Ω–∞–ª–æ–≥–∏ –¥–ª—è {ticker}:")
                    for i, pattern in enumerate(similar_patterns, 1):
                        trend = "‚Üë" if pattern['trend'] == 'up' else "‚Üì"
                        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã
                        date_str = pattern['date'].strftime('%Y-%m-%d') if hasattr(pattern['date'],
                                                                                   'strftime') else str(pattern['date'])
                        print(
                            f"   {i}. {date_str} (—Å—Ö–æ–¥—Å—Ç–≤–æ: {pattern['similarity']:.2f}) ‚Üí {trend} {pattern['future_return']:.2%}")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –∞–Ω–∞–ª–æ–≥–∏ –¥–ª—è {ticker}: {e}")

            all_predictions.append({
                'ticker': ticker,
                'lgb_growth_prob': growth_probs['growth_probs']['lgb'],
                'xgb_growth_prob': growth_probs['growth_probs']['xgb'],
                'lstm_growth_prob': growth_probs['growth_probs']['lstm'],
                'ensemble_growth_prob': growth_probs['growth_probs']['ensemble'],
                'current_price': growth_probs['current_price'],
                'predictions': growth_probs['price_forecast'],
                'ci_lower': growth_probs['ci_lower'],
                'ci_upper': growth_probs['ci_upper'],
                'expected_return': growth_probs['expected_return'],
                'expected_return_range': growth_probs['expected_return_range'],
                'test_dir_acc_lgb': dir_acc_lgb,
                'test_dir_acc_xgb': dir_acc_xgb,
                'test_dir_acc_lstm': dir_acc_lstm,
                'baseline_dir_acc': baseline_dir_acc,
                'shap_explanation': explanation['top_features'],
                'historical_analogues': similar_patterns
            })
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏ –¥–ª—è {ticker}: {e}")
            import traceback
            traceback.print_exc()
            continue
    # 9. –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if all_predictions:
        results_df = pd.DataFrame(all_predictions)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ–∂–∏–¥–∞–µ–º–æ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
        results_df_sorted = results_df.sort_values(by='expected_return', ascending=False).reset_index(drop=True)

        print("\n" + "üìä –ê–ù–ê–õ–ò–ó –ü–†–û–ì–ù–û–ó–û–í –ü–û –í–°–ï–ú –ê–ö–¶–ò–Ø–ú")
        print("=" * 110)

        # –§–∏–ª—å—Ç—Ä—É–µ–º –∞–∫—Ü–∏–∏ —Å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–π –æ–∂–∏–¥–∞–µ–º–æ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å—é
        positive_return = results_df_sorted[results_df_sorted['expected_return'] > 0]

        if positive_return.empty:
            top_3 = results_df_sorted.head(3)
            print("üìâ –ù–µ—Ç –∞–∫—Ü–∏–π —Å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–π –æ–∂–∏–¥–∞–µ–º–æ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å—é.")
            print("üõ°Ô∏è  –¢–û–ü-3 —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º–∏ –æ–∂–∏–¥–∞–µ–º—ã–º–∏ –ø–æ—Ç–µ—Ä—è–º–∏:")
        else:
            top_3 = positive_return.head(3)
            print("üèÜ –¢–û–ü-3 –ê–ö–¶–ò–ô –ö –ü–û–ö–£–ü–ö–ï –ù–ê –ë–õ–ò–ñ–ê–ô–®–ò–ï 15 –î–ù–ï–ô")
            print("üìå –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –æ–∂–∏–¥–∞–µ–º–æ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏: —á–µ–º –≤—ã—à–µ %, —Ç–µ–º –±–æ–ª—å—à–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –ø—Ä–∏–±—ã–ª—å")

        print("-" * 110)
        print(
            f"{'–ú–µ—Å—Ç–æ':<6} {'–¢–∏–∫–µ—Ä':<8} {'LGB':<6} {'XGB':<6} {'LSTM':<6} {'–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å':<12} {'–û–∂–∏–¥. –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å':<18}")
        print("-" * 110)

        for idx, (_, row) in enumerate(top_3.iterrows(), 1):
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ–∂–∏–¥–∞–µ–º—É—é –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å —Å –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º
            lower, upper = row['expected_return_range']
            return_str = f"{row['expected_return']:.2f}% [{lower:.2f}% - {upper:.2f}%]"

            print(f"{idx:<6} {row['ticker']:<8} "
                  f"{row['lgb_growth_prob']:>5.1f}% "
                  f"{row['xgb_growth_prob']:>5.1f}% "
                  f"{row['lstm_growth_prob']:>5.1f}% "
                  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º ensemble_growth_prob –≤–º–µ—Å—Ç–æ growth_probs['ensemble']
                  f"{row['ensemble_growth_prob']:>10.1f}% "
                  f"{return_str:<18}")

        print("-" * 110)

        # –ü–æ–ª–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥
        print("\nüìã –ü–û–õ–ù–´–ô –†–ï–ô–¢–ò–ù–ì –í–°–ï–• –ê–ö–¶–ò–ô (–æ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–æ–π –∫ –Ω–∞–∏–º–µ–Ω–µ–µ)")
        print("üí° –û–∂–∏–¥–∞–µ–º–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å ‚Äî –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã —á–µ—Ä–µ–∑ 15 –¥–Ω–µ–π")
        print("üìä –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å ‚Äî –¥–æ–ª—è –¥–Ω–µ–π —Å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å—é –≤ –ø—Ä–æ–≥–Ω–æ–∑–µ")
        print("-" * 110)
        print(
            f"{'–ú–µ—Å—Ç–æ':<6} {'–¢–∏–∫–µ—Ä':<8} {'–û–∂–∏–¥. –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å':<18} {'–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å':<12} {'LGB':<6} {'XGB':<6} {'LSTM':<6}")
        print("-" * 110)

        for idx, (_, row) in enumerate(results_df_sorted.iterrows(), 1):
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ–∂–∏–¥–∞–µ–º—É—é –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å —Å –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º
            lower, upper = row['expected_return_range']
            return_str = f"{row['expected_return']:.2f}% [{lower:.2f}% - {upper:.2f}%]"

            print(f"{idx:<6} {row['ticker']:<8} "
                  f"{return_str:<18} "
                  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º ensemble_growth_prob –≤–º–µ—Å—Ç–æ growth_probs['ensemble']
                  f"{row['ensemble_growth_prob']:>10.1f}% "
                  f"{row['lgb_growth_prob']:>5.1f}% "
                  f"{row['xgb_growth_prob']:>5.1f}% "
                  f"{row['lstm_growth_prob']:>5.1f}%")

        # –î–æ–±–∞–≤–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
        print("\nüîç –î–ï–¢–ê–õ–ò –ü–†–û–ì–ù–û–ó–ê –î–õ–Ø –¢–û–ü-3 –ê–ö–¶–ò–ô:")
        print("-" * 110)

        for idx, (_, row) in enumerate(top_3.iterrows(), 1):
            print(f"{idx}. {row['ticker']}")
            if row['shap_explanation']:
                print(f"   üí° –û—Å–Ω–æ–≤–Ω—ã–µ –¥—Ä–∞–π–≤–µ—Ä—ã –ø—Ä–æ–≥–Ω–æ–∑–∞: {', '.join(row['shap_explanation'])}")

            if isinstance(row['historical_analogues'], list) and len(row['historical_analogues']) > 0:
                print(f"   üîç –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –∞–Ω–∞–ª–æ–≥–∏:")
                for i, pattern in enumerate(row['historical_analogues'][:2], 1):
                    trend = "‚Üë" if pattern['trend'] == 'up' else "‚Üì"
                    date_str = pattern['date'].strftime('%Y-%m-%d') if hasattr(pattern['date'], 'strftime') else str(
                        pattern['date'])
                    print(
                        f"      {i}. {date_str} (—Å—Ö–æ–¥—Å—Ç–≤–æ: {pattern['similarity']:.2f}) ‚Üí {trend} {pattern['future_return']:.2%}")

            # –í—ã–≤–æ–¥–∏–º –æ–∂–∏–¥–∞–µ–º—É—é –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å —Å –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º
            lower, upper = row['expected_return_range']
            print(f"   üìà –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {row['expected_return']:.2f}%")
            print(f"   üìä 90% –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª: [{lower:.2f}% - {upper:.2f}%]")
            print(f"   üí∞ –¶–µ–Ω–∞: {row['current_price']:.2f} ‚Üí {row['predictions'][-1]:.2f}")
            print("-" * 110)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        results_df_sorted.to_csv('results/all_predictions.csv', index=False)
        top_3.to_csv('results/top_stocks.csv', index=False)
        print("\nüìä –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: all_predictions.csv, top_stocks.csv")
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ø—Ä–æ–≥–Ω–æ–∑—ã –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–π –∞–∫—Ü–∏–∏.")
    print("üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


if __name__ == "__main__":
    main()